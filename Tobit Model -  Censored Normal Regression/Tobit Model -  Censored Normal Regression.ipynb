{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Tobit Model -  Censored Normal Regression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xx529/Algorithm/blob/main/Tobit%20Model%20-%20%20Censored%20Normal%20Regression/Tobit%20Model%20-%20%20Censored%20Normal%20Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kof3fOwQ1TA"
      },
      "source": [
        "# Tobit Model -  Censored Normal Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaDaWXhxQ1TC"
      },
      "source": [
        "**关于算法的故事**\n",
        "\n",
        "在任一给定年份，有相当数量家庭的医疗保险费用支出为0\n",
        "\n",
        "因此，虽然年度家庭医疗保险费用支出的总体分布散布于一个很大的正数范围内，但在数字0上却相当集中"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq_7TwqFQ1TD"
      },
      "source": [
        "# Algorithm\n",
        "\n",
        "**定义**\n",
        "\n",
        "1. 因变量 $y$ 在正值上大致是连续分布，但也包含一部分以正概率取值为0的观测，对于 $y$ 如下定义，$y^*$ 是一个隐藏的变量，它与0共同决定了观测 $y$\n",
        "\n",
        "$$y=\\left\\{\n",
        "\\begin{aligned}\n",
        "y^* ,& y^* \\geqslant 0 \\\\\n",
        "- ,& y^* < 0 \n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "2. 隐藏变量 $y^*$ 由一个参数为 $\\beta$ 的线性方程和 $\\epsilon$，定义如下\n",
        "2\n",
        "$$y^* = x^T\\beta + \\epsilon, \\text{ 其中 } \\epsilon \\sim N(0, \\sigma^2)$$\n",
        "$$$$\n",
        "\n",
        "3. 需要优化的参数就是 $\\beta$ 和 $\\sigma$\n",
        "\n",
        "$$\\hat{\\theta}= (\\hat{\\beta}, \\hat{\\sigma}^2)$$\n",
        "\n",
        "**目标函数**\n",
        "\n",
        "1. 定义一个 indicator variable $d$，取 $L=0$ 如下:\n",
        "\n",
        "$$d=\\left\\{\n",
        "\\begin{aligned}\n",
        "1 ,& y > L\\\\\n",
        "0 ,& y = L \n",
        "\\end{aligned}\n",
        "\\right.$$\n",
        "\n",
        "2. 由定义式可知 $y^* \\sim (x^T\\beta, \\sigma^2)$\n",
        "\n",
        "$$y^* = x^T\\beta + \\epsilon \\rightarrow y^* \\sim (x^T\\beta, \\sigma^2)$$\n",
        "\n",
        "3. 当 $y^*<L$，用 CDF概率分布函数，当 $y^*>L$ 可观测时候用 PDF概率密度函数，这里取 $L=0$\n",
        "\n",
        "$$f(y)=\\left\\{\n",
        "\\begin{aligned} F^*(0)=1-\\Phi(\\frac{x^T\\beta}{\\sigma}), d=0\\\\\n",
        "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-x^T\\beta)^2), d=1\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "4. 综合上面两个分段函数就有如下形式，也就是我们通过 MLE 最大化的函数\n",
        "\n",
        "$$f(y) =[\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{1}{2\\sigma^2}(y-x^T\\beta)^2)]^d[1-\\Phi(\\frac{x^T\\beta}{\\sigma})]^{1-d}$$\n",
        "\n",
        "**参数梯度**\n",
        "\n",
        "对以上公式取 $log$ 之后的参数梯度，其中 $\\phi_i=\\phi(\\frac{x_i^T\\beta}{\\sigma})$，$\\Phi_i=\\Phi(\\frac{x_i^T\\beta}{\\sigma})$\n",
        "\n",
        "$$\\frac{\\partial ln L_N}{\\partial \\beta}=\\sum^N_{i=1}\\frac{1}{\\sigma^2}(d_i(y_i-x^T\\beta)-(1-d_i)\\frac{\\sigma\\phi_i}{1-\\Phi_i})x_i$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial ln L_N}{\\partial \\sigma^2}=\\sum^N_{i=1}[d_i(-\\frac{1}{2\\sigma^2}+\\frac{(y_i-x^T\\beta)^2}{2\\sigma^4})+(1-d_i)\\frac{\\phi_ix_i^T\\beta}{1-\\Phi_i} · \\frac{1}{2\\sigma^3}]$$\n",
        "\n",
        "**优化方法**\n",
        "\n",
        "[Proximal Methods](https://github.com/xx529/Algorithm/blob/main/Proximal%20Algorithms%20-%20L1%20Regularization/Proximal%20Algorithms%20-%20L1%20Regularization.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RiuZETpQ1TE"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, vmap, jit\n",
        "from jax.lax import cond\n",
        "import numpy as np"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPO65iEWQ1TF"
      },
      "source": [
        "# Experiment Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG_Q5KdWQ1TF",
        "outputId": "94679c01-245d-4206-ae3d-3b22033f6567"
      },
      "source": [
        "key = jax.random.PRNGKey(199)\n",
        "batch = 10000\n",
        "input_dim = 4\n",
        "L = 0\n",
        "\n",
        "true_beta = jnp.array([2.0, 3.0, -2.0, 0.0])\n",
        "true_sigma = 2.0\n",
        "epsilon = jax.random.normal(key, (batch, )) * (true_sigma**2)\n",
        "\n",
        "x = jax.random.normal(key, (batch, input_dim))\n",
        "y_star = jnp.dot(x, true_beta) + epsilon\n",
        "d = (y_star > L).astype(jnp.float32)\n",
        "y =  d * y_star\n",
        "\n",
        "beta = jax.random.normal(key, (input_dim, ))\n",
        "sigma = jax.random.normal(key)\n",
        "\n",
        "print('epsilon mean', epsilon.mean())\n",
        "print('epsilon std', epsilon.std())\n",
        "print('y_star std', y_star.std())\n",
        "print('x_dot_b mean', jnp.dot(x, true_beta).mean())\n",
        "print('y_star mean', y_star.mean())"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epsilon mean 0.0054394654\n",
            "epsilon std 3.9888053\n",
            "y_star std 5.7290034\n",
            "x_dot_b mean -0.030860584\n",
            "y_star mean -0.025421131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CviwImsBQ1TG",
        "outputId": "3c65f31f-94c2-4df4-f1b2-349e52c62080"
      },
      "source": [
        "y"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-0.       ,  2.0374558, 14.781202 , ..., -0.       ,\n",
              "             -0.       ,  3.0696194], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buaAoUKQQ1TG",
        "outputId": "9eeb5a89-6ccd-4fe5-ac1b-e4e51fa3c1bb"
      },
      "source": [
        "y_star"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-4.097417 ,  2.0374558, 14.781202 , ..., -4.7747993,\n",
              "             -3.3773355,  3.0696194], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKbjrrAeQ1TE"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnuFd-KVQ1TH"
      },
      "source": [
        "### General Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qPm6IEGQ1TH"
      },
      "source": [
        "def normal_cdf(x):\n",
        "    return jax.scipy.stats.norm.cdf(x, loc=0, scale=1)\n",
        "\n",
        "\n",
        "def normal_pdf(x):\n",
        "    return jax.scipy.stats.norm.pdf(x, loc=0, scale=1)\n",
        "\n",
        "\n",
        "def linear(x, beta):\n",
        "    return jnp.dot(x, beta)\n",
        "\n",
        "\n",
        "def log_mle(x, beta, sigma, y_true):\n",
        "    d = (y_star > L).astype(jnp.float32)\n",
        "    temp1 = d * jnp.log(1 / (jnp.sqrt(2 * jnp.pi * sigma**2))) + (-1 / (2 * sigma**2)) * (y_true - linear(x, beta))**2\n",
        "    temp2 = (1 - d) * jnp.log(1 - normal_cdf(linear(x, beta) / sigma) + 0.01)\n",
        "    return jnp.mean(temp1 + temp2)\n",
        "\n",
        "\n",
        "def tobit_model_grad(x, y_true, beta, sigma):\n",
        "    x_dot_beta = linear(x, beta)\n",
        "    residual = y_true - x_dot_beta\n",
        "    sigma_square = sigma**2\n",
        "    cdf = normal_cdf(x_dot_beta / sigma)\n",
        "    pdf = normal_pdf(x_dot_beta / sigma)\n",
        "    d = (y_true > L).astype(jnp.float32)\n",
        "    \n",
        "    d_beta_temp = d*residual - (1-d) * sigma * pdf / (1 - cdf + 0.01)\n",
        "    d_beta = - (1 / sigma_square) * jnp.dot(d_beta_temp, x) / x.shape[0]\n",
        "    \n",
        "    d_sigma2_temp_1 = d * (-1 / (2 * sigma_square) + (residual**2) / (2 * sigma**4))    \n",
        "    d_sigma2_temp_2 = 1 / (2 * sigma**3 ) * (1 - d) * (pdf * x_dot_beta) / (1 - cdf + 0.01)\n",
        "    d_sigma2 = - jnp.mean(d_sigma2_temp_1 + d_sigma2_temp_2)\n",
        "    \n",
        "    return d_beta, d_sigma2\n",
        "\n",
        "\n",
        "def tobit_model_train(x, y_true, beta, sigma, lr, esp, max_iter):\n",
        "    current_mle, current_esp, current_iter = jnp.inf, 1, 1\n",
        "\n",
        "    while current_iter <= max_iter and current_esp >= esp:\n",
        "        d_beta, d_sigma2 = jit(tobit_model_grad)(x, y, beta, sigma)\n",
        "\n",
        "        beta = beta - lr * d_beta\n",
        "        sigma = jnp.sqrt(sigma**2 - lr * d_sigma2)\n",
        "\n",
        "        mle = log_mle(x, beta, sigma, y_true)\n",
        "        current_esp = jnp.abs(mle - current_mle)\n",
        "        current_mle = mle\n",
        "        current_iter += 1\n",
        "\n",
        "    return beta, sigma"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I78MPsSPbSM"
      },
      "source": [
        "### JAX Version\n",
        "1. Proximal Method\n",
        "2. Accelerated by jit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRASvNkd74vF"
      },
      "source": [
        "def normal_cdf(x):\n",
        "    return jax.scipy.stats.norm.cdf(x, loc=0, scale=1)\n",
        "\n",
        "\n",
        "def normal_pdf(x):\n",
        "    return jax.scipy.stats.norm.pdf(x, loc=0, scale=1)\n",
        "\n",
        "\n",
        "def linear(x, beta):\n",
        "    return jnp.dot(x, beta)\n",
        "\n",
        "\n",
        "def log_mle(x, beta, sigma, y_true):\n",
        "    d = (y_star > L).astype(jnp.float32)\n",
        "    temp1 = d * jnp.log(1 / (jnp.sqrt(2 * jnp.pi * sigma**2))) + (-1 / (2 * sigma**2)) * (y_true - linear(x, beta))**2\n",
        "    temp2 = (1 - d) * jnp.log(1 - normal_cdf(linear(x, beta) / sigma) + 0.01)\n",
        "    return jnp.mean(temp1 + temp2)\n",
        "\n",
        "\n",
        "@jax.partial(vmap, in_axes=(0, 0, None, None, None))\n",
        "def select_grad(x, y_true, beta, sigma, L):\n",
        "    return jax.lax.cond(\n",
        "        y_true > L,\n",
        "        lambda _: ( # > L\n",
        "            - 1 / (sigma**2) * jnp.dot((y_true - linear(x, beta)), x), # beta\n",
        "            - (- 1 / (2*sigma**2) + (y_true - linear(x, beta))**2 / (2 * sigma**4)) # sigma\n",
        "        ), \n",
        "        lambda _: ( # < L\n",
        "             1 / (sigma**2) * jnp.dot((sigma * normal_pdf(linear(x, beta) / sigma) / (1 - normal_cdf(linear(x, beta) / sigma) + 0.01)), x), # beta\n",
        "            - 1 / (2 * sigma**3) * normal_pdf(linear(x, beta) / sigma) * linear(x, beta) / (1 - normal_cdf(linear(x, beta) / sigma) + 0.01) # sigma\n",
        "        ), \n",
        "        operand = None\n",
        "    )\n",
        "\n",
        "\n",
        "def jax_soft_threshold(w, threshold):\n",
        "    return jax.lax.cond(\n",
        "        w > threshold, \n",
        "        lambda _: w - threshold,\n",
        "        lambda _: jax.lax.cond(\n",
        "            w < - threshold,\n",
        "            lambda _: w + threshold,\n",
        "            lambda _: 0.0,\n",
        "            None\n",
        "            ),\n",
        "        None\n",
        "        )\n",
        "    \n",
        "\n",
        "def jax_proximal_method_update(beta, sigma, x, y_true, l, lr, penalty):\n",
        "    d_beta, d_sigma2 = select_grad(x, y_true, beta, sigma, l)\n",
        "    w_beta = beta - lr * jnp.mean(d_beta, axis=0)\n",
        "    w_sigma = jnp.sqrt(sigma**2 - lr * jnp.mean(d_sigma2))\n",
        "\n",
        "    return vmap(jax_soft_threshold, in_axes=(0, None))(w_beta, lr * penalty), jax_soft_threshold(w_sigma, lr * penalty)\n",
        "\n",
        "\n",
        "def cond_fun(val):\n",
        "    x, y_true, beta, sigma, lr, L, penalty, current_iter, max_iter, current_mle, current_esp, esp = val\n",
        "    result = (val[7] <= val[8]) \n",
        "    # result = (val[8] > val[9]) \n",
        "    return result\n",
        "\n",
        "\n",
        "def body_fun(val):\n",
        "    x, y_true, beta, sigma, lr, L, penalty, current_iter, max_iter, current_mle, current_esp, esp = val\n",
        "\n",
        "    new_beta, new_sigma = jax_proximal_method_update(beta, sigma, x, y_true, L, lr, penalty)\n",
        "\n",
        "    new_mle = log_mle(x, new_beta, new_sigma, y_true)\n",
        "    current_esp = jnp.abs(current_mle - new_mle)\n",
        "    current_iter += 1\n",
        "\n",
        "    return (x, y_true, new_beta, new_sigma, lr, L, penalty, current_iter, max_iter, new_mle, current_esp, esp)\n",
        "\n",
        "\n",
        "@jit\n",
        "def jax_tobit_model_train(init_val):\n",
        "    return jax.lax.while_loop(cond_fun, body_fun, init_val)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAfAlWfFiX-g"
      },
      "source": [
        "# Code Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjj4DjJy8ed7"
      },
      "source": [
        "lr = 1e-3\n",
        "esp = 1e-6\n",
        "max_iter = 30000\n",
        "current_iter = 1\n",
        "current_mle = log_mle(x, beta, sigma, y)\n",
        "last_mle = log_mle(x, beta, sigma, y) + 1\n",
        "current_esp = jnp.abs(current_mle - last_mle) \n",
        "penalty = 0.03"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-JEBdQ9ZbE",
        "outputId": "dba608fa-0cf2-4f8f-f021-d1dcd7208ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(true_beta)\n",
        "print(true_sigma)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.  3. -2.  0.]\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTRcGjGL8jSf",
        "outputId": "d85a6e0f-0209-4133-e670-8df435dcbbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "result = tobit_model_train(x, y, beta, sigma, lr, esp, max_iter) # slow version\n",
        "print(result[0])\n",
        "print(result[1])"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.8833185   2.8528786  -1.7675685   0.08224886]\n",
            "2.5721285\n",
            "CPU times: user 7min 11s, sys: 1min 29s, total: 8min 41s\n",
            "Wall time: 6min 1s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-4r_neVYZSx",
        "outputId": "56dae546-02f3-415c-831d-e4364da1d5c6"
      },
      "source": [
        "%%time\n",
        "init_val = (x, y, beta, sigma, lr, L, 0.0, current_iter, max_iter, current_mle, current_esp, esp) # fast version\n",
        "result = jax_tobit_model_train(init_val)\n",
        "print(result[2])\n",
        "print(result[3])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.8833185   2.8528786  -1.7675683   0.08224886]\n",
            "2.5721283\n",
            "CPU times: user 3.76 s, sys: 764 ms, total: 4.52 s\n",
            "Wall time: 4.52 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flp5EAzkYZ93",
        "outputId": "76f00a53-fe19-44dc-c715-62d66374b70a"
      },
      "source": [
        "%%time\n",
        "init_val = (x, y, beta, sigma, lr, L, penalty, current_iter, max_iter, current_mle, current_esp, esp) # with penalty\n",
        "result = jax_tobit_model_train(init_val)\n",
        "print(result[2])\n",
        "print(result[3])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.6582375  2.6266336 -1.54973    0.       ]\n",
            "2.1737754\n",
            "CPU times: user 3.62 s, sys: 892 ms, total: 4.51 s\n",
            "Wall time: 4.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VdyukgBBHbF"
      },
      "source": [
        "# Others\n",
        "检查当 ϵ 为柯西分布时对整个估计的影响"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W79VUwFABJZB"
      },
      "source": [
        "key = jax.random.PRNGKey(300)\n",
        "batch = 10000\n",
        "input_dim = 6\n",
        "L = 0\n",
        "\n",
        "true_beta = jnp.array([2.0, 3.0, -2.0, 0.0])\n",
        "true_sigma = 2.0\n",
        "epsilon = jax.random.normal(key, (batch, )) * (true_sigma**2)\n",
        "\n",
        "x = jax.random.normal(key, (batch, input_dim))\n",
        "y_star = jnp.dot(x, true_beta) + epsilon\n",
        "d = (y_star > L).astype(jnp.float32)\n",
        "y =  d * y_star\n",
        "\n",
        "beta = jax.random.normal(key, (input_dim, ))\n",
        "sigma = jax.random.normal(key)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}