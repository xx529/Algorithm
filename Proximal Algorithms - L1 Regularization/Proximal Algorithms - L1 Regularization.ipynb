{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Proximal Algorithms - L1 Regularization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xx529/ML-Algorithm-Implementation/blob/main/Proximal%20Algorithms%20-%20L1%20Regularization/Proximal%20Algorithms%20-%20L1%20Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOfoWLd9QM7E"
      },
      "source": [
        "# Proximal Algorithms - L1 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWx1EjSQM7H"
      },
      "source": [
        "# Algorithm explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsOJEfk-QM7I"
      },
      "source": [
        "1. 采用 L1 正则的求逻辑回归的参数梯度\n",
        "\n",
        "$$\\underset{\\theta}{\\text{min}} \\sum_i^N(y_i-\\sigma(x_i^T\\theta))^2 + \\lambda||\\theta||_1$$\n",
        "\n",
        "\n",
        "2. 简略算法原理\n",
        "\n",
        "    1. 利用 proxmial operator 方法求解，其定义\n",
        "        \n",
        "        $$\\text{prox}_f(v) = \\underset{x}{\\text{argmin}}(f(x)+\\frac{1}{2}||x-v||^2_2)$$\n",
        "        \n",
        "    2. 对于 proxmial operator 有一个良好的性质\n",
        "        \n",
        "        $$ x^* = \\underset{x}{\\text{argmin}} f(x) \\text{    当且仅当    } x^* = prox_f(x^*)$$\n",
        "        \n",
        "    3. 对于一个函数 $z$ 由一个可微函数 $f$ 和一个不可微分函数 $g$ 组成，对于 $z$ 有如下定义重要公式\n",
        "        \n",
        "        $$z(x) = f(x) + g(x)$$\n",
        "        \n",
        "        $$x^{k+1} := \\text{prox}_{\\lambda^kg}(x^k - \\lambda^k \\nabla f(x^k))$$\n",
        "\n",
        "    4. 以上更新公式实际就是求解下式子\n",
        "        $$\\theta^*=\\underset{\\theta}{\\text{argmin}}f(\\theta)+g(\\theta)$$\n",
        "        \n",
        "    4. 应用在参数L1参数求解当中 $prox_f(x)$ 中的 $x$ 就是我门需要求的 $\\theta$， $\\lambda^k$ 是学习步长\n",
        "        \n",
        "        $$w:=\\theta^k-\\lambda^k\\nabla f_{\\theta^k}(X)$$\n",
        "        \n",
        "        $$\\theta^{k+1} = \\text{prox}_{{\\lambda^k}g}(w)$$\n",
        "\n",
        "    5. 令 $g(\\theta)=\\lambda||\\theta||_1$ 代入 proximal operator 定义式得，其中 $\\lambda$ 就是惩罚系数 penalty\n",
        "        \n",
        "        $$\\theta^{k+1}=\\text{prox}_{\\lambda^kg}(w)=\\underset{\\theta}{\\text{argmin}}(\\lambda^k \\lambda||\\theta||_1+\\frac{1}{2}||\\theta-w||_2^2)$$\n",
        "        \n",
        "    6. 求解上面式子的结果，$\\lambda$ 是 L1 正则的惩罚系数，$\\lambda^k$ 是学习步长，$\\theta$ 是一个向量\n",
        "\n",
        "$$ [\\theta^{k+1}]_i=\\left\\{\n",
        "\\begin{matrix}\n",
        "w_i - \\lambda\\lambda^k, & \\text{ if } & w_i > \\lambda\\lambda^k\\\\\n",
        "        0, & \\text{ if } & \\lambda\\lambda^k < w_i < -\\lambda\\lambda^k\\\\\n",
        "        w_i + \\lambda\\lambda^k, & \\text{ if } & w_i < -\\lambda\\lambda^k\\\\\n",
        "\\end{matrix}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "3. 详细证明过程附件中"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhOLcTqTQM7J"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUWorlrxQM7K"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "from jax import grad, vmap"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlxF1DDHQM7K",
        "outputId": "6a87f58d-b0a4-4e73-db2e-72339c150dc9"
      },
      "source": [
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "true_theta = jnp.array([2.0, 2.0, 0.0, 0.0, 0.0, 5.0])\n",
        "x = jax.random.normal(key, (1000, len(true_theta)))\n",
        "theta = jax.random.normal(key, (len(true_theta), ))\n",
        "y = (jnn.sigmoid(jnp.dot(x, true_theta)) >= 0.5).astype(jnp.float32)\n",
        "\n",
        "print('generated theta', true_theta)\n",
        "print('initial theta', theta)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated theta [2. 2. 0. 0. 0. 5.]\n",
            "initial theta [ 0.18784384 -1.2833426   0.6494181   1.2490593   0.24447003 -0.11744965]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xug4d6ZqQM7L"
      },
      "source": [
        "# Code Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1p5xmxEQM7L"
      },
      "source": [
        "## Normal Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiKbBswoQM7M"
      },
      "source": [
        "def predict(theta, x):\n",
        "    return jnn.sigmoid(jnp.dot(x, theta))\n",
        "\n",
        "def get_lr_grad(theta, x, y_true):\n",
        "    y_pre = predict(theta, x)\n",
        "    return jnp.dot(x.T, y_pre - y_true) / x.shape[0]\n",
        "\n",
        "def get_lr_loss(theta, x, y_true):\n",
        "    y_pre = predict(theta, x)\n",
        "    return -jnp.mean(y_true * jnp.log(y_pre) + (1 - y_true) * jnp.log(1 - y_pre))\n",
        "\n",
        "\n",
        "def soft_threshold(vec_theta, threshold):\n",
        "    new_vec_theta = []\n",
        "\n",
        "    for theta in vec_theta:\n",
        "        if theta > threshold:\n",
        "            new_vec_theta.append(theta - threshold)\n",
        "        elif theta < -threshold:\n",
        "            new_vec_theta.append(theta + threshold)\n",
        "        else:\n",
        "            new_vec_theta.append(0)\n",
        "    \n",
        "    return jnp.array(new_vec_theta)\n",
        "    \n",
        "\n",
        "def proximal_method_update(theta, x, y_true, lr, penalty):\n",
        "    w = theta - lr * get_lr_grad(theta, x, y_true)\n",
        "    new_theta = soft_threshold(w, lr * penalty)\n",
        "\n",
        "    return jnp.array(new_theta)\n",
        "\n",
        "\n",
        "def model_train(theta, x, y_true, lr, penalty, esp, max_iter):\n",
        "    current_iter = 1\n",
        "    current_esp = 1\n",
        "    current_loss = jnp.inf\n",
        "    \n",
        "    while max_iter > current_iter and current_esp > esp:\n",
        "        \n",
        "        new_loss = get_lr_loss(theta, x, y_true) + penalty * jnp.linalg.norm(theta, 1)\n",
        "        new_theta = proximal_method_update(theta, x, y_true, lr, penalty)\n",
        "        current_esp = jnp.abs(new_loss - current_loss)\n",
        "        \n",
        "        theta = new_theta\n",
        "        current_loss = new_loss\n",
        "        current_iter += 1\n",
        "        \n",
        "        # print('{}  loss: {:.4f} theta: {}'.format(current_iter-1, loss, theta))\n",
        "\n",
        "    return new_theta"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDyaRdFHQM7M",
        "outputId": "1d9a70c2-2926-41b1-f013-ce246075b2eb"
      },
      "source": [
        "%time\n",
        "lr = 0.05\n",
        "penalty = 0.1\n",
        "esp = 1e-4\n",
        "max_iter = 3000\n",
        "\n",
        "proximal_method_theta = model_train(theta, x, y, lr, penalty, esp, max_iter)\n",
        "proximal_method_theta"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0.20016596, 0.12244455, 0.        , 0.        , 0.        ,\n",
              "             1.0061318 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "d_k6pYjVQM7N"
      },
      "source": [
        "## Jax Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pMUID0sQM7N"
      },
      "source": [
        "def jax_loss(theta, x, y_true):\n",
        "    y_pre = jnn.sigmoid(jnp.dot(x, theta))\n",
        "    return -(jnp.mean(y*jnp.log(y_pre) + (1-y)*jnp.log(1-y_pre)))\n",
        "\n",
        "def jax_lr_grad(theta, x, y_true):\n",
        "    y_pre = jnn.sigmoid(jnp.dot(x, theta))\n",
        "    return jnp.dot(x.T, -(y_true - y_pre)) / x.shape[0]\n",
        "\n",
        "@jax.partial(vmap, in_axes=(0, None))\n",
        "def jax_soft_threshold(w, threshold):\n",
        "    return jax.lax.cond(\n",
        "        w > threshold, \n",
        "        lambda _: w - threshold,\n",
        "        lambda _: jax.lax.cond(\n",
        "            w < - threshold,\n",
        "            lambda _: w + threshold,\n",
        "            lambda _: 0.0,\n",
        "            None\n",
        "            ),\n",
        "        None\n",
        "        )\n",
        "    \n",
        "def jax_proximal_method_update(theta, x, y_true, lr, penalty):\n",
        "    w = theta - lr * jax_lr_grad(theta, x, y_true)\n",
        "    return jax_soft_threshold(w, lr * penalty)\n",
        "\n",
        "def jax_model_train(theta, x, y_true, lr, penalty, esp, max_iter):\n",
        "    current_iter = 1\n",
        "    current_esp = 1\n",
        "    current_loss = jnp.inf\n",
        "    \n",
        "    while max_iter > current_iter and current_esp > esp:\n",
        "        \n",
        "        new_loss = jax_loss(theta, x, y_true) + penalty * jnp.linalg.norm(theta, 1)\n",
        "        new_theta = jax_proximal_method_update(theta, x, y_true, lr, penalty)\n",
        "        current_esp = jnp.abs(new_loss - current_loss)\n",
        "        \n",
        "        theta = new_theta\n",
        "        current_loss = new_loss\n",
        "        current_iter += 1\n",
        "        # print('{}  loss: {:.4f} theta: {}'.format(current_iter-1, current_loss, theta))\n",
        "\n",
        "    return new_theta"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fLqJw-zcGPT",
        "outputId": "b4b85d51-340e-4074-ab47-0c6efcfca13a"
      },
      "source": [
        "%time\n",
        "lr = 0.05\n",
        "penalty = 0.1\n",
        "esp = 1e-4\n",
        "max_iter = 100000\n",
        "\n",
        "jax_proximal_method_theta = model_train(theta, x, y, lr, penalty, esp, max_iter)\n",
        "jax_proximal_method_theta"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([0.20016596, 0.12244455, 0.        , 0.        , 0.        ,\n",
              "             1.0061318 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}